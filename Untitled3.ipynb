{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sghosh32/NCG-Task/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ncg-task/training-data.git\n",
        "!git clone https://github.com/ncg-task/test-data.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqCDrlLvikaH",
        "outputId": "599d2ba9-2c3c-41de-9968-ef0719010ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'training-data'...\n",
            "remote: Enumerating objects: 6864, done.\u001b[K\n",
            "remote: Counting objects: 100% (3083/3083), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2728/2728), done.\u001b[K\n",
            "remote: Total 6864 (delta 567), reused 2504 (delta 279), pack-reused 3781\u001b[K\n",
            "Receiving objects: 100% (6864/6864), 157.36 MiB | 28.79 MiB/s, done.\n",
            "Resolving deltas: 100% (660/660), done.\n",
            "Updating files: 100% (3286/3286), done.\n",
            "Cloning into 'test-data'...\n",
            "remote: Enumerating objects: 2508, done.\u001b[K\n",
            "remote: Total 2508 (delta 0), reused 0 (delta 0), pack-reused 2508\u001b[K\n",
            "Receiving objects: 100% (2508/2508), 215.28 MiB | 18.97 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n",
            "Updating files: 100% (2060/2060), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVOJ0QE_UVUl",
        "outputId": "91eb7fad-d508-4bc7-e043-4e2f878bf64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchdata) (2.25.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.9/dist-packages (from torchdata) (1.26.14)\n",
            "Collecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.9/dist-packages (from torchdata) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->torchdata) (4.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchdata) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->torchdata) (4.0.0)\n",
            "Installing collected packages: portalocker, torchdata\n",
            "Successfully installed portalocker-2.7.0 torchdata-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.utils.data import Dataset\n",
        "import torch.optim as optim\n",
        "from torchdata import DataLoader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import BertTokenizer\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "H-Sp2FueICjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Notebook is running on\", device)\n",
        "\n",
        "SEED = 4444\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "GLlj1A8LIFo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sentences, tokenizer, max_len):\n",
        "        self.sentences = sentences\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        sentence = str(self.sentences[item])\n",
        "        print(type(sentence))\n",
        "        encoding = self.tokenizer.encode_plus(sentence,\n",
        "            self.sentences,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'sentence': sentence,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(0, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "0FfifaliS7UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "    ds = Dataset(\n",
        "        sentences=df,\n",
        "        tokenizer=tokenizer,\n",
        "        max_len=max_len\n",
        "    )\n",
        "\n",
        "    return torch.utils.data.DataLoader(\n",
        "        ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False\n",
        "    )\n",
        "train_path = '/content/training-data/'\n",
        "test_path = '/content/test-data/'\n",
        "\n",
        "train_loader = create_data_loader(train_path,BertTokenizer,512,1)\n",
        "test_loader = create_data_loader(test_path,BertTokenizer,512,1)"
      ],
      "metadata": {
        "id": "BohlVlZuVWKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_dimension, n_heads, dropout):\n",
        "        super(MultiHeadAttentionLayer, self).__init__()\n",
        "        self.hidden_dimension = hidden_dimension\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dimension = hidden_dimension // n_heads\n",
        "        self.fc_Q = nn.Linear(hidden_dimension, hidden_dimension)\n",
        "        self.fc_K = nn.Linear(hidden_dimension, hidden_dimension)\n",
        "        self.fc_V = nn.Linear(hidden_dimension, hidden_dimension)\n",
        "        self.fc_O = nn.Linear(hidden_dimension, hidden_dimension)\n",
        "        self.scale = math.sqrt(self.head_dimension)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        batch_size = query.shape[0]\n",
        "        Q = self.fc_Q(query)\n",
        "        K = self.fc_K(key)\n",
        "        V = self.fc_V(value)\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dimension).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dimension).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dimension).permute(0, 2, 1, 3)\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        output = self.fc_O(x.view(batch_size, -1, self.hidden_dimension))\n",
        "        return output"
      ],
      "metadata": {
        "id": "hYupvbNwVpta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hidden_dimension, pf_dimension, dropout):\n",
        "        super(PositionwiseFeedforwardLayer, self).__init__()\n",
        "        self.fc_1 = nn.Linear(hidden_dimension, pf_dimension)\n",
        "        self.fc_2 = nn.Linear(pf_dimension, hidden_dimension)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        x = self.fc_2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Hc7n3GfyVtnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, hidden_dimension, n_heads, pf_dimension, dropout, device):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attention_layer_norm = nn.LayerNorm(hidden_dimension)\n",
        "        self.positionwise_feedforward_layer_norm = nn.LayerNorm(hidden_dimension)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hidden_dimension, n_heads, dropout)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hidden_dimension, pf_dimension, dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        src = self.self_attention_layer_norm(src + self.dropout(_src))\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        src = self.positionwise_feedforward_layer_norm(src + self.dropout(_src))\n",
        "        return src"
      ],
      "metadata": {
        "id": "KuZf_5bbVxK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dimension, hidden_dimension, n_layers, n_heads, pf_dimension, dropout, device, max_length = 100):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.device = device\n",
        "        self.tok_embedding = nn.Embedding(input_dimension, hidden_dimension)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hidden_dimension)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hidden_dimension, n_heads, pf_dimension, dropout, device) for _ in range(n_layers)])\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hidden_dimension])).to(device)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        return src"
      ],
      "metadata": {
        "id": "GxqTN3_xV4PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder_Layer(nn.Module):\n",
        "    def __init__(self, hidden_dimension, n_heads, pff_dimension, dropout):\n",
        "        super(Decoder_Layer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttentionLayer(hidden_dimension, n_heads, dropout)\n",
        "        self.cross_attention = MultiHeadAttentionLayer(hidden_dimension, n_heads, dropout)\n",
        "        self.pff = PositionwiseFeedforwardLayer(hidden_dimension, pff_dimension, dropout)\n",
        "        self.attention_norm1 = nn.LayerNorm(hidden_dimension)\n",
        "        self.attention_norm2 = nn.LayerNorm(hidden_dimension)\n",
        "        self.pff_normalized = nn.LayerNorm(hidden_dimension)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, target, target_mask, encoder_output, source_mask):\n",
        "        self_attention = self.self_attention(target, target, target, target_mask)\n",
        "        output1 = self.attention_norm1(self.dropout(self_attention) + target)\n",
        "        cross_attention = self.cross_attention(output1, encoder_output, encoder_output, source_mask)\n",
        "        output2 = self.attention_norm2(self.dropout(cross_attention) + output1)\n",
        "        pff_output = self.pff(output2)\n",
        "        output = self.pff_normalized(self.dropout(pff_output) + output2)\n",
        "        return output "
      ],
      "metadata": {
        "id": "fNQby_9qRX1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, token_vocab_size, positional_vocab_size, hidden_dimension, decoder_heads, decoder_pff_dimension, num_layers, decoder_dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(token_vocab_size, hidden_dimension)\n",
        "        self.positional_embedding = nn.Embedding(positional_vocab_size, hidden_dimension)\n",
        "        self.decoder_layers = nn.ModuleList([Decoder_Layer(hidden_dimension, decoder_heads, decoder_pff_dimension, decoder_dropout) for i in range(num_layers)])\n",
        "        self.fc = nn.Linear(hidden_dimension, token_vocab_size)\n",
        "        self.scale = math.sqrt(hidden_dimension)\n",
        "        self.dropout = nn.Dropout(decoder_dropout)\n",
        "\n",
        "    def forward(self, target, target_mask, encoder_output, source_mask):\n",
        "        batch_size = target.shape[0]\n",
        "        target_length = target.shape[1]\n",
        "        token_embedding = self.token_embedding(target)\n",
        "        positional_tensor = torch.arange(0, target_length).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
        "        positional_embedding = self.positional_embedding(positional_tensor)\n",
        "        decoder_embedding = self.dropout(token_embedding * self.scale + positional_embedding)\n",
        "        decoder_state = decoder_embedding\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            decoder_state = decoder_layer(decoder_state, target_mask, encoder_output, source_mask)\n",
        "        output = self.fc(decoder_state)\n",
        "        return output"
      ],
      "metadata": {
        "id": "XrNcTKMYRYm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, source_padding_index, target_padding_index):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.source_padding_index = source_padding_index\n",
        "        self.target_padding_index = target_padding_index\n",
        "\n",
        "    def make_source_mask(self, src):\n",
        "        source_mask = (src != self.source_padding_index).unsqueeze(1).unsqueeze(2).to(device)\n",
        "        return source_mask\n",
        "\n",
        "    def make_target_mask(self, trg):\n",
        "        trg_length = trg.shape[1]\n",
        "        pad_mask = (trg != self.target_padding_index).unsqueeze(1).unsqueeze(2).to(device)\n",
        "        sub_mask = torch.tril(torch.ones((trg_length, trg_length), device = device)).bool()\n",
        "        target_mask = pad_mask & sub_mask\n",
        "        return target_mask\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        source_mask = self.make_source_mask(source)\n",
        "        target_mask = self.make_target_mask(target)\n",
        "        encoder_output = self.encoder(source, source_mask)\n",
        "        output = self.decoder(target, target_mask, encoder_output, source_mask)\n",
        "        return output"
      ],
      "metadata": {
        "id": "5klVBiXPRnYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DNN(nn.Module):\n",
        "    def init(self, input_dimension, hidden_dimension, output_dimension, dropout):\n",
        "        super(DNN, self).init()\n",
        "        self.fc_1 = nn.Linear(input_dimension, hidden_dimension)\n",
        "        self.fc_2 = nn.Linear(hidden_dimension, hidden_dimension)\n",
        "        self.fc_3 = nn.Linear(hidden_dimension, output_dimension)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        x = self.dropout(torch.relu(self.fc_2(x)))\n",
        "        x = self.fc_3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "frxz6DdKNEm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 10\n",
        "LR = 0.0005\n",
        "CLIP = 1\n",
        "SOURCE_VOCAB_SIZE = 513\n",
        "HIDDEN_DIMENSION = 256\n",
        "ENCODER_PFF_DIMENSION = 512\n",
        "ENCODER_HEADS = 8\n",
        "ENCODER_DROPOUT = 0.1\n",
        "ENCODER_NUM_LAYERS = 3\n",
        "MAX_LENGTH = 256"
      ],
      "metadata": {
        "id": "zs4D0m2naxN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(y_true, y_pred):\n",
        "    recall = recall_score(y_true, y_pred, average = 'macro')\n",
        "    precision = precision_score(y_true, y_pred, average = 'macro')\n",
        "    f1 = f1_score(y_true, y_pred, average = 'macro')\n",
        "    return recall, precision, f1"
      ],
      "metadata": {
        "id": "26LgztxrPlJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Train(iterator, model, criterion, optimizer, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(iterator):\n",
        "        optimizer.zero_grad()\n",
        "        source = batch.src\n",
        "        target = batch.trg\n",
        "        outputs = model(source, target[:, :-1])\n",
        "        outputs = outputs.contiguous().view(-1, outputs.shape[-1])\n",
        "        targets = target[:, 1:].contiguous().view(-1).to(device)\n",
        "        batch_loss = criterion(outputs, targets)\n",
        "        batch_loss.backward()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += batch_loss.item()\n",
        "    return epoch_loss / len(iterator)\n"
      ],
      "metadata": {
        "id": "qDp9ZUQ6aZ2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Evaluate(iterator, model, criterion):\n",
        "    model.eval()\n",
        "    eval_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for _, batch in enumerate(iterator):\n",
        "            source = batch.src\n",
        "            target = batch.trg\n",
        "            outputs = model(source, target[:, :-1])\n",
        "            outputs = outputs.contiguous().view(-1, outputs.shape[-1])\n",
        "            targets = target[:, 1:].contiguous().view(-1).to(device)\n",
        "            batch_loss = criterion(outputs, targets)\n",
        "            eval_loss += batch_loss.item()\n",
        "        return eval_loss/len(iterator)"
      ],
      "metadata": {
        "id": "zdquC26tR6ur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(SOURCE_VOCAB_SIZE, MAX_LENGTH, HIDDEN_DIMENSION, ENCODER_HEADS, ENCODER_PFF_DIMENSION, ENCODER_NUM_LAYERS, ENCODER_DROPOUT).to(device)\n",
        "decoder = Decoder(TARGET_VOCAB_SIZE, MAX_LENGTH, HIDDEN_DIMENSION, DECODER_HEADS, DECODER_PFF_DIMENSION, DECODER_NUM_LAYERS, DECODER_DROPOUT).to(device)\n",
        "transformer = Transformer(encoder, decoder, source_padding_index, target_padding_index).to(device)\n",
        "dnn_model = DNN(512, HIDDEN_DIMENSION, 16, 0.2))\n",
        "optimizer = optim.Adam(transformer.parameters(), LR)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "AI4Jk0Fne-tH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "transformer.apply(initialize_weights)"
      ],
      "metadata": {
        "id": "sB_uhweISUQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Learning Rate: {LR}, Hidden Dimmensions: {HIDDEN_DIMENSION}\")\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "prev_epoch = 1\n",
        "min_losses = [float('inf'), float('inf')]\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    train_loss = Train(train_loader, encoder, criterion, optimizer, CLIP)\n",
        "    train_losses.append(train_loss)\n",
        "    valid_loss = Evaluate(test_loader, transformer, criterion)\n",
        "    valid_losses.append(valid_loss)\n",
        "    if valid_loss < min_losses[0]:\n",
        "        min_losses[0] = valid_loss\n",
        "        min_losses[1] = train_loss\n",
        "    if epoch % int(NUM_EPOCHS / 10) == 0:\n",
        "        prev_epoch = epoch + 1\n",
        "        print(f\"Training Loss: {train_loss:.4f} | Validation Loss: {valid_loss:.4f}\")\n",
        "        print(f\"Training PPL: {math.exp(train_loss):.4f} | Validation PPL: {math.exp(valid_loss):.4f}\")"
      ],
      "metadata": {
        "id": "7UftTN8Uasoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.eval()\n",
        "test_loss = Evaluate(test_loader, transformer, criterion)"
      ],
      "metadata": {
        "id": "ztr7mcGNS3m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_model.train()\n",
        "epoch_loss = 0\n",
        "for i, batch in enumerate(outs):\n",
        "    optimizer.zero_grad()\n",
        "    source = batch.src\n",
        "    target = batch.trg\n",
        "    outputs = dnn_model(source, target[:, :-1])\n",
        "    outputs = outputs.contiguous().view(-1, outputs.shape[-1])\n",
        "    targets = target[:, 1:].contiguous().view(-1).to(device)\n",
        "    _,_,acc = calculate_metrics(targets,outputs.detach())\n",
        "    loss = torch.tensor(acc,requires_grad=True)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(dnn_model.parameters(), CLIP)\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()"
      ],
      "metadata": {
        "id": "HPu1jedRdxxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_model.eval()\n",
        "eval_loss = 0\n",
        "with torch.no_grad():\n",
        "    for _, batch in enumerate(batch):\n",
        "        source = batch.src\n",
        "        target = batch.trg\n",
        "        outputs = dnn_model(source, target[:, :-1])\n",
        "        outputs = outputs.contiguous().view(-1, outputs.shape[-1])\n",
        "        targets = target[:, 1:].contiguous().view(-1).to(device)\n",
        "        _,_,acc = calculate_metrics(targets,outputs.detach())\n",
        "        loss = torch.tensor(acc,requires_grad=True)\n",
        "        eval_loss += loss.item()"
      ],
      "metadata": {
        "id": "UQ7ozu1yeBUo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}